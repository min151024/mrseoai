import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from datetime import datetime, timedelta
from serp_api_utils import get_top_competitor_urls, get_meta_info_from_url
from chatgpt_utils import build_prompt, get_chatgpt_response
import os
import base64
from googleapiclient.errors import HttpError
from flask import render_template
from ga_utils import fetch_ga_data


# ==========
# èªè¨¼éƒ¨åˆ†
# ==========
SERVICE_ACCOUNT_FILE = 'credentials.json'

# Renderä¸Šã«ç’°å¢ƒå¤‰æ•°ãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ credentials.json ã‚’ä½œæˆ
if "GOOGLE_CREDS_BASE64" in os.environ:
    with open(SERVICE_ACCOUNT_FILE, "wb") as f:
        f.write(base64.b64decode(os.getenv("GOOGLE_CREDS_BASE64")))

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "credentials.json"

SPREADSHEET_ID = '1Fpdb-3j89j7OkPmJXbdmSmFBaA6yj2ZB0AUBNvF6BQ4'
SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']
SHEET_SCOPES = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# gspreadç”¨ã®èªè¨¼
credentials = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, SHEET_SCOPES)
gc = gspread.authorize(credentials)

# Google Sheets APIç”¨ã®èªè¨¼
creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=["https://www.googleapis.com/auth/spreadsheets"])
service = build("sheets", "v4", credentials=creds)

def fetch_date(gsc_data):
    """Google Search Consoleãƒ‡ãƒ¼ã‚¿ã¨GAãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€"""

    # Google Analytics ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    today = datetime.today().date()
    start_date = (today - timedelta(days=7)).isoformat()
    end_date = today.isoformat()
    ga_data = fetch_ga_data(start_date, end_date)

    # Spreadsheet èªè¨¼ã¨å–å¾—
    spreadsheet = gc.open_by_key(SPREADSHEET_ID)

    # ã€ŒSEO_Dataã€ã‚·ãƒ¼ãƒˆã®å–å¾—ï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰
    try:
        sheet = spreadsheet.worksheet("é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸")
        print("âœ… ã€é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã€ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except gspread.exceptions.WorksheetNotFound:
        sheet = spreadsheet.add_worksheet(title="SEO_Data", rows="100", cols="20")
        print("ğŸ†• ã€é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã€ã‚·ãƒ¼ãƒˆã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")

    # ã‚¯ãƒªã‚¢
    sheet.clear()

    # GSCãƒ˜ãƒƒãƒ€ãƒ¼
    gsc_values = [["URL", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "å¹³å‡é †ä½", "ã‚¯ãƒªãƒƒã‚¯æ•°", "è¡¨ç¤ºå›æ•°"]]

    # GSCãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„å ´åˆã«è¿½åŠ 
    if gsc_data:
        print(f"ğŸ“Š GSCãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(gsc_data)}")
        for row in gsc_data:
            gsc_values.append([
                row.get("url", "ãªã—"),
                row.get("query", "ãªã—"),
                row.get("position", "ãªã—"),
                row.get("clicks", "ãªã—"),
                row.get("impressions", "ãªã—")
            ])
    else:
        print("âš ï¸ GSCãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã—ãŸã€‚")
        gsc_values.append(["ãƒ‡ãƒ¼ã‚¿ãªã—", "", "", "", ""])

    # GSCãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆA1ã‹ã‚‰ï¼‰
    sheet.update(values=gsc_values, range_name="A1")

    # GAãƒ˜ãƒƒãƒ€ãƒ¼
    ga_values = [["æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "æµå…¥çµŒè·¯", "ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", "ã‚¤ãƒ™ãƒ³ãƒˆæ•°", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"]]

    if ga_data is None or ga_data.empty:
        print("âš ï¸ Google Analytics ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        ga_values.append(["ãƒ‡ãƒ¼ã‚¿ãªã—", "", "", "", ""])
    else:
        for _, row in ga_data.iterrows():
            ga_values.append([
                row.get("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãªã—"),
                row.get("æµå…¥çµŒè·¯", "ãªã—"),
                row.get("ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", 0),
                row.get("ã‚¤ãƒ™ãƒ³ãƒˆæ•°", 0),
                row.get("ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°", 0)
            ])

    # GAãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆG1ã‹ã‚‰ï¼‰
    sheet.update(values=ga_values, range_name="G1")

    print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¾ã—ãŸï¼")


def fetch_data(service, site_url, start_date, end_date):
    """Google Search Console ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    request = {
        'startDate': start_date.isoformat(),
        'endDate': end_date.isoformat(),
        'dimensions': ['query', 'page'],
        'rowLimit': 1000
    }
    try:
        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
        rows = response.get('rows', [])

        # ãƒ‡ãƒ¼ã‚¿ã‚’ pandas ã® DataFrame ã«å¤‰æ›
        data = []
        for row in rows:
            data.append([
                row['keys'][0],  # URL
                row.get('clicks', 0),
                row.get('impressions', 0),
                row.get('ctr', 0) * 100,  # CTRï¼ˆ% ã«å¤‰æ›ï¼‰
                row.get('position', 0)
            ])

        df = pd.DataFrame(data, columns=['URL', 'ã‚¯ãƒªãƒƒã‚¯æ•°', 'è¡¨ç¤ºå›æ•°', 'CTRï¼ˆ%ï¼‰', 'å¹³å‡é †ä½'])
        return df
    except HttpError as error:
        print(f"âŒ Google Search Console API ã‚¨ãƒ©ãƒ¼: {error}")
        return pd.DataFrame(columns=['URL', 'ã‚¯ãƒªãƒƒã‚¯æ•°', 'è¡¨ç¤ºå›æ•°', 'CTRï¼ˆ%ï¼‰', 'å¹³å‡é †ä½'])


    
#----------------------------------------------------------

def process_seo_improvement(site_url):
    """æŒ‡å®šã—ãŸURLã®SEOæ”¹å–„ã‚’å®Ÿè¡Œ"""
    print(f"ğŸš€ SEOæ”¹å–„ã‚’é–‹å§‹: {site_url}")

    # Google Search Console API èªè¨¼
    credentials = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, SCOPES)
    service = build('searchconsole', 'v1', credentials=credentials)

    # æ—¥ä»˜è¨­å®šï¼ˆéå»7æ—¥é–“ã¨ãã®å‰ã®7æ—¥é–“ï¼‰
    today = datetime.today().date()
    this_week_start = today - timedelta(days=7)
    this_week_end = today
    last_week_start = today - timedelta(days=14)
    last_week_end = today - timedelta(days=7)

    # Google Search Console ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_this_week = fetch_data(service, site_url, this_week_start, this_week_end)
    df_last_week = fetch_data(service, site_url, last_week_start, last_week_end)

    print("last_week rows:", len(df_last_week))
    print("this_week rows:", len(df_this_week))



    print(df_this_week.head())

    # Google Sheets å–å¾—
    spreadsheet = gc.open_by_key(SPREADSHEET_ID)

    try:
        sheet_suggestions = spreadsheet.worksheet("æ”¹å–„æ¡ˆ")
        print("âœ… æ—¢å­˜ã®ã€æ”¹å–„æ¡ˆã€ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except gspread.exceptions.WorksheetNotFound:
        sheet_suggestions = spreadsheet.add_worksheet(title="æ”¹å–„æ¡ˆ", rows="100", cols="20")
        print("ğŸ†• æ–°ã—ãã€æ”¹å–„æ¡ˆã€ã‚·ãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä»Šé€±ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã‚€
    sheet_suggestions.clear()
    sheet_suggestions.append_row(['URL', 'ã‚¯ãƒªãƒƒã‚¯æ•°', 'è¡¨ç¤ºå›æ•°', 'CTRï¼ˆ%ï¼‰', 'å¹³å‡é †ä½'])
    for row in df_this_week.values.tolist():
        sheet_suggestions.append_row(row)

    gsc_data_for_export = []
    for _, row in df_this_week.iterrows():
     gsc_data_for_export.append({
        "url": row["URL"],
        "query": "ï¼ˆä»®ï¼‰",  # query ãŒãªã„ã®ã§ä»®ã«åŸ‹ã‚ã‚‹
        "position": row["å¹³å‡é †ä½"],
        "clicks": row["ã‚¯ãƒªãƒƒã‚¯æ•°"],
        "impressions": row["è¡¨ç¤ºå›æ•°"]
    })

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãå‡ºã—å®Ÿè¡Œ
    fetch_date(gsc_data_for_export)

    # é †ä½å¤‰åŒ–ã‚’è¨ˆç®—
    merged_df = pd.merge(df_last_week, df_this_week, on='URL', suffixes=('_å…ˆé€±', '_ä»Šé€±'))
    merged_df['é †ä½å¤‰åŒ–'] = merged_df['å¹³å‡é †ä½_ä»Šé€±'] - merged_df['å¹³å‡é †ä½_å…ˆé€±']
    dropped_df = merged_df[merged_df['é †ä½å¤‰åŒ–'] > 0].sort_values(by='é †ä½å¤‰åŒ–', ascending=False)

# ğŸ’¡ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã€Œé †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã€ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°
    try:
      sheet_dropped = spreadsheet.worksheet("é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸")
      print("âœ… ã€é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã€ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except gspread.exceptions.WorksheetNotFound:
      sheet_dropped = spreadsheet.add_worksheet(title="é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸", rows="100", cols="20")
      print("ğŸ†• ã€é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã€ã‚·ãƒ¼ãƒˆã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")

# ä¸Šæ›¸ãã§ã‚¯ãƒªã‚¢ï¼†ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
    sheet_dropped.clear()
    sheet_dropped.append_row(['URL', 'å¹³å‡é †ä½ï¼ˆå…ˆé€±ï¼‰', 'å¹³å‡é †ä½ï¼ˆä»Šé€±ï¼‰', 'é †ä½å¤‰åŒ–'])
    for _, row in dropped_df.iterrows():
        sheet_dropped.append_row([
            row['URL'],
            row['å¹³å‡é †ä½_å…ˆé€±'],
            row['å¹³å‡é †ä½_ä»Šé€±'],
            row['é †ä½å¤‰åŒ–']
        ])

    if dropped_df.empty:
        print("âŒ é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # é †ä½ãŒæœ€ã‚‚ä½ã„ãƒšãƒ¼ã‚¸ã‚’ä»£ã‚ã‚Šã«é¸ã¶
        all_df = pd.merge(df_last_week, df_this_week, on='URL', suffixes=('_å…ˆé€±', '_ä»Šé€±'))
        worst_page = all_df.sort_values(by='å¹³å‡é †ä½_ä»Šé€±', ascending=False).iloc[0]
        target_url = worst_page['URL']
        print(f"ğŸ¯ é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ãŒãªã„ãŸã‚ã€å¹³å‡é †ä½ãŒæœ€ã‚‚æ‚ªã„ãƒšãƒ¼ã‚¸ã‚’å¯¾è±¡ã«è¨­å®š: {target_url}")
    else:
        # é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã®ä¸­ã‹ã‚‰1ãƒšãƒ¼ã‚¸ã‚’é¸ã¶
        target_url = dropped_df.iloc[0]['URL']
        print(f"ğŸ¯ é †ä½ãŒä¸‹ãŒã£ãŸãƒšãƒ¼ã‚¸ã®ä¸­ã‹ã‚‰å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’è¨­å®š: {target_url}")

    # ãƒ¡ã‚¿æƒ…å ±ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
    try:
        meta_info = get_meta_info_from_url(target_url)
        keyword = meta_info.get("title") or meta_info.get("description") or "SEO"
        print(f"ğŸ” æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
    except Exception as e:
        print(f"âš ï¸ ãƒ¡ã‚¿æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
        return


    # ç«¶åˆãƒšãƒ¼ã‚¸å–å¾—
    try:
        top_urls = get_top_competitor_urls(keyword)
        competitors_info = [get_meta_info_from_url(url) for url in top_urls if url]
    except Exception as e:
        print(f"âš ï¸ ç«¶åˆãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
        competitors_info = []

    # ChatGPT ã«æ”¹å–„æ¡ˆã‚’ä¾é ¼
    try:
        prompt = build_prompt(target_url, competitors_info)
        response = get_chatgpt_response(prompt)
        print("ğŸ’¡ ChatGPTæ”¹å–„æ¡ˆ:\n", response)
    except Exception as e:
        print(f"âš ï¸ ChatGPTã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {e}")

        response = "ChatGPT ã‹ã‚‰ã®æ”¹å–„ææ¡ˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        result_html = f"""
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>SEO æ”¹å–„ææ¡ˆ</title>
    </head>
    <body>
        <h2>SEO æ”¹å–„ææ¡ˆ</h2>
        <p><strong>å¯¾è±¡URLï¼š</strong> {target_url}</p>
        <h3>ğŸ’¡ ChatGPTã®æ”¹å–„ææ¡ˆ</h3>
        <p>{response}</p>
        <a href="/">æˆ»ã‚‹</a>
    </body>
    </html>
    """
    data = df_this_week.values.tolist() 
    table_html = "<table border='1'><tr><th>URL</th><th>æ¤œç´¢ã‚¯ã‚¨ãƒª</th><th>ã‚¯ãƒªãƒƒã‚¯æ•°</th><th>è¡¨ç¤ºå›æ•°</th><th>CTR</th><th>å¹³å‡é †ä½</th></tr>"
    for row in data:
        table_html += "<tr>" + "".join(f"<td>{cell}</td>" for cell in row) + "</tr>"
    table_html += "</table>"


    # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã¦å‡ºåŠ›
    data = df_this_week.values.tolist() if df_this_week is not None else []

# `result.html` ã«ä¿å­˜
    with open("templates/result.html", "w", encoding="utf-8") as f:
      f.write(result_html)

    print("âœ… HTMLãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä»Šé€±ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã‚€
    if df_this_week is not None and not df_this_week.empty:
     sheet_suggestions.clear()
     sheet_suggestions.append_row(['URL', 'ã‚¯ãƒªãƒƒã‚¯æ•°', 'è¡¨ç¤ºå›æ•°', 'CTRï¼ˆ%ï¼‰', 'å¹³å‡é †ä½'])
     for row in df_this_week.values.tolist():
        sheet_suggestions.append_row(row)
    else:
     print("âŒ Google Search Console ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    return result_html

if __name__ == "__main__":
    process_seo_improvement("https://mrseoai.com")