from datetime import date, timedelta
from urllib.parse import urlparse
import pandas as pd

from serp_api_utils import get_top_competitor_urls, get_meta_info_from_url
from chatgpt_utils import build_prompt, get_chatgpt_response

from ga_utils import fetch_ga_conversion_for_url
from gsc_utils import fetch_gsc_data
from sheet_utils import (
    get_spreadsheet, get_or_create_worksheet,
    update_sheet, write_competitor_data_to_sheet
)

# Firestore ã¯ firebase_admin ã§çµ±ä¸€ï¼ˆgoogle.cloud ã¨æ··åœ¨ã•ã›ãªã„ï¼‰
from firebase_admin import firestore as fa_firestore
db = fa_firestore.client()


# ---------------------- ãƒ˜ãƒ«ãƒ‘ãƒ¼ ----------------------

def _last_28_days():
    """æ˜¨æ—¥ã¾ã§ã®ç›´è¿‘28æ—¥"""
    ed = date.today() - timedelta(days=1)
    sd = ed - timedelta(days=27)
    return sd.isoformat(), ed.isoformat()

def _path_only(u: str) -> str:
    """URLã§ã‚‚ãƒ‘ã‚¹ã§ã‚‚OK â†’ å…ˆé ­ãŒ / ã®ãƒ‘ã‚¹ã«æ­£è¦åŒ–"""
    if not u:
        return "/"
    if u.startswith("/"):
        return u or "/"
    p = urlparse(u if u.startswith(("http://", "https://")) else "https://" + u)
    return p.path or "/"

# ---------------------- ãƒ¡ã‚¤ãƒ³ï¼ˆæ–°ã‚·ã‚°ãƒãƒãƒ£ï¼‰ ----------------------

def process_seo_improvement(
    *,
    url: str,                      # ãƒ•ã‚©ãƒ¼ãƒ ã§å…¥åŠ›ã•ã‚ŒãŸãƒ•ãƒ«URLï¼ˆè¡¨ç¤ºç”¨ï¼‰
    creds=None,                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼OAuth Credentialsï¼ˆç„¡ã‘ã‚Œã° Noneï¼‰
    sc_property: str | None = None,# "sc-domain:example.com" or "https://example.com/"
    ga_property: str | int | None = None,   # "properties/123..." or 123 or None
    sheet_id: str | None = None,   # å‡ºåŠ›å…ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼ˆä»»æ„ï¼‰
    skip_metrics: bool = False,    # True ãªã‚‰ GA/GSC/Sheets ã‚’ä¸¸ã”ã¨ã‚¹ã‚­ãƒƒãƒ—
) -> dict:
    print(f"ğŸš€ SEOæ”¹å–„ã‚’é–‹å§‹: {url} (skip_metrics={skip_metrics})")

    # ---------------- 1) GSC / GA æŒ‡æ¨™ã®åé›† ----------------
    gsc_df = pd.DataFrame()
    merged_df = pd.DataFrame()
    chart_labels, chart_data = [], {}
    clicks = impressions = conversions = 0
    ctr = position = 0.0

    if not skip_metrics and creds:
        sd, ed = _last_28_days()

        # ---- GSC ----
        if sc_property:
            try:
                raw = fetch_gsc_data(
                    creds=creds,
                    sc_property=sc_property,
                    start_date=sd,
                    end_date=ed,
                    row_limit=25000,
                )  # åˆ—: ['æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰','URL','ã‚¯ãƒªãƒƒã‚¯æ•°','è¡¨ç¤ºå›æ•°','CTRï¼ˆ%ï¼‰','å¹³å‡é †ä½']
                # URLå˜ä½ã«é›†è¨ˆï¼ˆé‡è¤‡ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ãŸã‚ï¼‰
                if not raw.empty:
                    gsc_df = (
                        raw.groupby("URL", as_index=False)
                           .agg({
                               "ã‚¯ãƒªãƒƒã‚¯æ•°": "sum",
                               "è¡¨ç¤ºå›æ•°": "sum",
                               "CTRï¼ˆ%ï¼‰": "mean",
                               "å¹³å‡é †ä½": "mean",
                           })
                    )
            except Exception as e:
                print("GSCå–å¾—ã‚¹ã‚­ãƒƒãƒ—:", e)

        # ---- GAï¼ˆã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¾‹ï¼‰----
        ga_conv = pd.DataFrame(columns=["URL", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"])
        if ga_property and not gsc_df.empty:
            per_url = []
            for u in gsc_df["URL"].unique():
                try:
                    ga_df = fetch_ga_conversion_for_url(
                        creds=creds,
                        ga_property=ga_property,
                        start_date=sd,
                        end_date=ed,
                        full_url=_path_only(u),
                    )  # åˆ—: ['URL','ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°']
                    if not ga_df.empty:
                        per_url.append(ga_df.iloc[0])
                except Exception as e:
                    print("GAå–å¾—ã‚¹ã‚­ãƒƒãƒ—(å˜ãƒšãƒ¼ã‚¸):", u, e)
            if per_url:
                ga_conv = pd.DataFrame(per_url)
            else:
                ga_conv = pd.DataFrame(columns=["URL", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"])

        # ---- ãƒãƒ¼ã‚¸ ----
        if not gsc_df.empty:
            merged_df = pd.merge(gsc_df, ga_conv, on="URL", how="left")
            merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"] = merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].fillna(0).astype(int)

            # ãƒãƒ£ãƒ¼ãƒˆãƒ»é›†è¨ˆ
            chart_labels = merged_df["URL"].tolist()
            chart_data = {
                "clicks":      merged_df["ã‚¯ãƒªãƒƒã‚¯æ•°"].astype(int).tolist(),
                "impressions": merged_df["è¡¨ç¤ºå›æ•°"].astype(int).tolist(),
                "ctr":         merged_df["CTRï¼ˆ%ï¼‰"].astype(float).round(2).tolist(),
                "position":    merged_df["å¹³å‡é †ä½"].astype(float).round(2).tolist(),
                "conversions": merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].astype(int).tolist(),
            }

            clicks       = int(merged_df["ã‚¯ãƒªãƒƒã‚¯æ•°"].sum())
            impressions  = int(merged_df["è¡¨ç¤ºå›æ•°"].sum())
            conversions  = int(merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].sum())
            ctr          = float(merged_df["CTRï¼ˆ%ï¼‰"].mean())
            position     = float(merged_df["å¹³å‡é †ä½"].mean())

    # ---------------- 2) ç«¶åˆå–å¾— & ChatGPT ææ¡ˆ ----------------
    # GSCã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ã‚Œã°ï¼‰ã‹ã‚‰ä»£è¡¨ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—
    gsc_keywords = []
    try:
        if not skip_metrics and "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in locals().get("raw", pd.DataFrame()).columns:
            gsc_keywords = [
                str(k).strip() for k in raw["æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].dropna().tolist()
                if str(k).strip()
            ]
    except Exception:
        pass

    competitors_info = []
    competitor_data  = []
    response         = ""  # ç”Ÿæˆå¤±æ•—æ™‚ã¯ç©ºã®ã¾ã¾

    if gsc_keywords:
        try:
            top_urls = get_top_competitor_urls(gsc_keywords[0]) or []
        except Exception as e:
            print("SerpAPIå‘¼ã³å‡ºã—å¤±æ•—:", e)
            top_urls = []

        for idx, u in enumerate(top_urls, start=1):
            try:
                info = get_meta_info_from_url(u)
            except Exception:
                info = {}
            competitors_info.append(info)
            competitor_data.append({
                "URL": u,
                "ã‚¿ã‚¤ãƒˆãƒ«": info.get("title", ""),
                "ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³": info.get("description", ""),
                "position": idx,
                "title": info.get("title", ""),
                "url": u,
            })

        if competitors_info:
            try:
                prompt = build_prompt(
                    # æ”¹å–„å¯¾è±¡ï¼šé †ä½ãŒæ‚ªã„URLã€‚ãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ URLã®ãƒ«ãƒ¼ãƒˆã€‚
                    (merged_df.sort_values("å¹³å‡é †ä½", ascending=False).iloc[0]["URL"]
                     if not merged_df.empty else url.rstrip("/") + "/"),
                    competitors_info,
                    merged_df if not merged_df.empty else pd.DataFrame(),
                )
                response = get_chatgpt_response(prompt) or ""
            except Exception as e:
                print("ChatGPTç”Ÿæˆå¤±æ•—:", e)

    # ---------------- 3) Sheets æ›¸ãè¾¼ã¿ï¼ˆä»»æ„ï¼‰ ----------------
    if (not skip_metrics) and creds and sheet_id:
        try:
            spreadsheet = get_spreadsheet(creds, sheet_id)
            ws = get_or_create_worksheet(spreadsheet, "SEOãƒ‡ãƒ¼ã‚¿")

            if not merged_df.empty:
                headers = ["URL", "ã‚¯ãƒªãƒƒã‚¯æ•°", "è¡¨ç¤ºå›æ•°", "CTRï¼ˆ%ï¼‰", "å¹³å‡é †ä½", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"]
                rows = merged_df[headers].values.tolist()
            else:
                headers = ["URL", "ã‚¯ãƒªãƒƒã‚¯æ•°", "è¡¨ç¤ºå›æ•°", "CTRï¼ˆ%ï¼‰", "å¹³å‡é †ä½", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"]
                rows = []

            update_sheet(ws, headers, rows)
            write_competitor_data_to_sheet(spreadsheet, competitor_data)
            print("âœ… Sheets æ›¸ãè¾¼ã¿å®Œäº†")
        except Exception as e:
            print("Sheetsæ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—:", e)

    # ---------------- 4) ãƒ†ãƒ¼ãƒ–ãƒ«HTML ----------------
    if not merged_df.empty:
        table_html = merged_df.to_html(classes="table table-sm", index=False)
    else:
        table_html = "<p>ç›´è¿‘28æ—¥ã§æœ‰åŠ¹ãªGSC/GAãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</p>"

    # ---------------- 5) ãƒ¬ã‚¹ãƒãƒ³ã‚¹ ----------------
    return {
        "clicks":           clicks,
        "impressions":      impressions,
        "ctr":              ctr,
        "position":         position,
        "conversions":      conversions,
        "table_html":       table_html,
        "chart_labels":     chart_labels,
        "chart_data":       chart_data,
        "competitors":      competitor_data,
        "chatgpt_response": response or "",
    }


def get_history_for_user(uid: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ï¼ˆæ–°ã—ã„é †ï¼‰"""
    docs = (
        db.collection("improvements")
          .where("uid", "==", uid)
          .order_by("timestamp", direction=fa_firestore.Query.DESCENDING)
          .stream()
    )
    history = []
    for doc in docs:
        d = doc.to_dict()
        ts = d.get("timestamp")
        if hasattr(ts, "strftime"):
            ts_str = ts.strftime("%Y-%m-%d %H:%M:%S")
        else:
            ts_str = str(ts)
        history.append({
            "id":               doc.id,
            "timestamp":        ts_str,
            "input_url":        d.get("input_url", ""),
            "chatgpt_response": d.get("result", {}).get("chatgpt_response", ""),
        })
    return history


if __name__ == "__main__":
    process_seo_improvement("sc-domain:mrseoai.com")
