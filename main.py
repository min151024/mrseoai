import pandas as pd
from datetime import datetime, timedelta
from urllib.parse import urlparse
from serp_api_utils import get_top_competitor_urls, get_meta_info_from_url
from chatgpt_utils import build_prompt, get_chatgpt_response
from ga_utils import fetch_ga_conversion_for_url
from gsc_utils import get_search_console_service, fetch_gsc_data
from sheet_utils import (
    get_spreadsheet,
    get_or_create_worksheet,
    update_sheet,
    write_competitor_data_to_sheet
)
from google.cloud import firestore
db = firestore.Client()


SPREADSHEET_ID = '1Fpdb-3j89j7OkPmJXbdmSmFBaA6yj2ZB0AUBNvF6BQ4'

def extract_path_from_url(full_url: str) -> str:
    parsed_url = urlparse(full_url)
    return parsed_url.path or "/"

def process_seo_improvement(site_url, skip_metrics: bool = False):
    print(f"\U0001F680 SEOæ”¹å–„ã‚’é–‹å§‹: {site_url}")

    if skip_metrics:
        df_this_week = pd.DataFrame() 
    else:
        today = datetime.today().date()
        yesterday = today - timedelta(days=2)
        this_week_start = yesterday - timedelta(days=6)
        this_week_end   = yesterday

        service = get_search_console_service()
        df_this_week = fetch_gsc_data(service, site_url, this_week_start, this_week_end)


    print("âœ… ä»Šé€±ã®df:")
    print(df_this_week)

    clicks       = 0
    impressions  = 0
    ctr          = 0.0
    position     = 0
    conversions  = 0
    table_html   = ""
    chart_labels = []
    chart_data   = []
    merged_df    = pd.DataFrame() 

    if df_this_week.empty:
        print("âŒ ä»Šé€±ã®GSCãƒ‡ãƒ¼ã‚¿ãŒç©ºãªã®ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†ã—ã¾ã™ã€‚")
        table_html = "<p>ä»Šé€±ã®GSC/GAãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ã‚µãƒ¼ãƒ“ã‚¹ç´¹ä»‹æ–‡ã¨ç«¶åˆæƒ…å ±ã‚’å…ƒã«æ”¹å–„æ¡ˆã‚’ä½œæˆã—ã¾ã™ã€‚</p>"

    else:
        ga_conversion_data = []
        for url in df_this_week["URL"].unique():
            page_path = urlparse(url).path or "/"
            ga_df = fetch_ga_conversion_for_url(
                start_date=this_week_start.isoformat(),
                end_date=this_week_end.isoformat(),
                full_url=page_path
            )
            if not ga_df.empty:
                ga_conversion_data.append(ga_df.iloc[0])
        if ga_conversion_data:
            ga_df_combined = pd.DataFrame(ga_conversion_data)
        else:
            ga_df_combined = pd.DataFrame(columns=["URL", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"])

        merged_df = pd.merge(df_this_week, ga_df_combined, on="URL", how="left")
        merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"] = merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].fillna(0).astype(int)

        print("ğŸ” merged_df ã®ä¸­èº«:")
        print(merged_df)

        # â€”â€“ ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†è¨ˆ â€”â€“
        clicks      = int(merged_df['ã‚¯ãƒªãƒƒã‚¯æ•°'].sum())
        impressions = int(merged_df['è¡¨ç¤ºå›æ•°'].sum())
        ctr         = float(merged_df['CTRï¼ˆ%ï¼‰'].mean())
        position    = float(merged_df['å¹³å‡é †ä½'].mean())
        conversions = int(merged_df['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°'].sum())

        # â€”â€“ ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ & ãƒ†ãƒ¼ãƒ–ãƒ«HTML â€”â€“
        chart_labels = merged_df["URL"].tolist()
        chart_data = {
            "clicks":      merged_df["ã‚¯ãƒªãƒƒã‚¯æ•°"].tolist(),
            "impressions": merged_df["è¡¨ç¤ºå›æ•°"].tolist(),
            "ctr":         merged_df["CTRï¼ˆ%ï¼‰"].tolist(),
            "position":    merged_df["å¹³å‡é †ä½"].tolist(),
            "conversions": merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].tolist()
        }
        table_html   = merged_df.to_html(classes="table table-sm", index=False)

    # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLé¸å®šï¼ˆmerged_df ãŒç©ºãªã‚‰ã‚µã‚¤ãƒˆTOPï¼‰
    if not merged_df.empty:
        worst_page = merged_df.sort_values(by='å¹³å‡é †ä½', ascending=False).iloc[0]
        target_url = worst_page['URL']
    else:
        target_url = site_url.rstrip("/") + "/"
    print(f"ğŸ¯ æ”¹å–„å¯¾è±¡ãƒšãƒ¼ã‚¸: {target_url}")

    # 4. ç«¶åˆæƒ…å ±å–å¾— & ChatGPT æ”¹å–„æ¡ˆå‘¼ã³å‡ºã—ï¼ˆå…±é€šï¼‰
    try:
        meta_info = get_meta_info_from_url(target_url)
        keyword   = meta_info.get("title") or meta_info.get("description") or "SEO"
    except:
        keyword = "SEO"

    try:
        top_urls         = get_top_competitor_urls(keyword)
        competitors_info = [get_meta_info_from_url(u) for u in top_urls if u]
    except:
        competitors_info = []

    competitor_data = []
    for idx, u in enumerate(top_urls, start=1):
        info = get_meta_info_from_url(u)
        competitor_data.append({
         "URL":                 u,
         "ã‚¿ã‚¤ãƒˆãƒ«":            info.get("title",""),
         "ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³": info.get("description","")
     })


    try:
        prompt = build_prompt(target_url, competitors_info, merged_df)
        response = get_chatgpt_response(prompt)
        print("ğŸ’¡ ChatGPTæ”¹å–„æ¡ˆ:", response)
    except Exception as e:
        print(f"âš ï¸ ChatGPTã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {e}")
        response = "ChatGPT ã‹ã‚‰ã®æ”¹å–„ææ¡ˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"


    spreadsheet = get_spreadsheet(SPREADSHEET_ID)
    sheet_result = get_or_create_worksheet(spreadsheet, "SEOãƒ‡ãƒ¼ã‚¿")

    print("ğŸ“¤ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")
    update_sheet(sheet_result, merged_df.columns.tolist(), merged_df.values.tolist())
    write_competitor_data_to_sheet(spreadsheet, competitor_data)
    print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
        

    html_rows = ""
    for _, row in merged_df.iterrows():
       html_rows += f"<tr><td>{row['URL']}</td><td>{row['ã‚¯ãƒªãƒƒã‚¯æ•°']}</td><td>{row['è¡¨ç¤ºå›æ•°']}</td><td>{row['CTRï¼ˆ%ï¼‰']}</td><td>{row['å¹³å‡é †ä½']}</td><td>{row['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°']}</td></tr>"

    total_clicks = merged_df['ã‚¯ãƒªãƒƒã‚¯æ•°'].sum()
    total_impressions = merged_df['è¡¨ç¤ºå›æ•°'].sum()
    overall_ctr = (total_clicks / total_impressions) * 100 if total_impressions > 0 else 0
    average_position = merged_df['å¹³å‡é †ä½'].mean()
    total_conversions = merged_df['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°'].sum()

    clicks      = int(total_clicks)
    impressions = int(total_impressions)
    ctr         = float(overall_ctr)
    position    = float(average_position)
    conversions = int(total_conversions)
    table_html   = html_rows
    chart_labels = merged_df["URL"].tolist()
    data = {
        "clicks":      merged_df["ã‚¯ãƒªãƒƒã‚¯æ•°"].tolist(),
        "impressions": merged_df["è¡¨ç¤ºå›æ•°"].tolist(),
        "ctr":         merged_df["CTRï¼ˆ%ï¼‰"].tolist(),
        "position":    merged_df["å¹³å‡é †ä½"].tolist(),
        "conversions": merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].tolist()
    }

    return {
        "clicks":           clicks,
        "impressions":      impressions,
        "ctr":              ctr,
        "position":         position,
        "conversions":      conversions,
        "table_html":       table_html,
        "chart_labels":     chart_labels,
        "chart_data":       data,
        "competitors":      competitor_data,
        "chatgpt_response": response or ""
    }

def get_history_for_user(uid):
    docs = (
        db.collection("improvements")
          .where("uid", "==", uid)
          .order_by("timestamp", direction=firestore.Query.DESCENDING)
          .stream()
    )
    history = []
    for doc in docs:
        d = doc.to_dict()
        history.append({
            "id":               doc.id,
            "timestamp":        d["timestamp"].strftime("%Y-%m-%d %H:%M:%S"),
            "input_url":        d["input_url"],
            "chatgpt_response": d["result"]["chatgpt_response"]
        })
    return history

if __name__ == "__main__":
    process_seo_improvement("sc-domain:mrseoai.com")
