import pandas as pd
from datetime import datetime, timedelta
from urllib.parse import urlparse
from serp_api_utils import get_top_competitor_urls, get_meta_info_from_url
from chatgpt_utils import build_prompt, get_chatgpt_response
from ga_utils import fetch_ga_conversion_for_url
from gsc_utils import get_search_console_service, fetch_gsc_data
from sheet_utils import (
    get_spreadsheet,
    get_or_create_worksheet,
    update_sheet,
    write_competitor_data_to_sheet
)
from google.cloud import firestore
db = firestore.Client()


SPREADSHEET_ID = '1Fpdb-3j89j7OkPmJXbdmSmFBaA6yj2ZB0AUBNvF6BQ4'

def extract_path_from_url(full_url: str) -> str:
    parsed_url = urlparse(full_url)
    return parsed_url.path or "/"

def process_seo_improvement(site_url, skip_metrics: bool = False):
    print(f"\U0001F680 SEOæ”¹å–„ã‚’é–‹å§‹: {site_url}")

    if skip_metrics:
        df_this_week = pd.DataFrame() 
    else:
        today = datetime.today().date()
        yesterday = today - timedelta(days=2)
        this_week_start = yesterday - timedelta(days=6)
        this_week_end   = yesterday

        service = get_search_console_service()
        df_this_week = fetch_gsc_data(service, site_url, this_week_start, this_week_end)


    print("âœ… ä»Šé€±ã®df:")
    print(df_this_week)

    clicks       = 0
    impressions  = 0
    ctr          = 0.0
    position     = 0
    conversions  = 0
    table_html   = ""
    chart_labels = []
    chart_data   = []
    merged_df    = pd.DataFrame() 

    if df_this_week.empty:
        print("âŒ ä»Šé€±ã®GSCãƒ‡ãƒ¼ã‚¿ãŒç©ºãªã®ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†ã—ã¾ã™ã€‚")
        merged_df = pd.DataFrame(columns=[
        "URL", "ã‚¯ãƒªãƒƒã‚¯æ•°", "è¡¨ç¤ºå›æ•°", "CTRï¼ˆ%ï¼‰", "å¹³å‡é †ä½", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"
        ])
        table_html = "<p>ä»Šé€±ã®GSC/GAãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™â€¦</p>"

    else:
        ga_conversion_data = []
        for url in df_this_week["URL"].unique():
            page_path = urlparse(url).path or "/"
            ga_df = fetch_ga_conversion_for_url(
                start_date=this_week_start.isoformat(),
                end_date=this_week_end.isoformat(),
                full_url=page_path
            )
            if not ga_df.empty:
                ga_conversion_data.append(ga_df.iloc[0])
        if ga_conversion_data:
            ga_df_combined = pd.DataFrame(ga_conversion_data)
        else:
            ga_df_combined = pd.DataFrame(columns=["URL", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"])

        merged_df = pd.merge(df_this_week, ga_df_combined, on="URL", how="left")
        merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"] = merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].fillna(0).astype(int)

        print("ğŸ” merged_df ã®ä¸­èº«:")
        print(merged_df)

        if merged_df.empty:
            clicks = 0
            impressions = 0
            ctr = 0
            position = 0
            conversions = 0
        else:
            clicks      = int(merged_df['ã‚¯ãƒªãƒƒã‚¯æ•°'].sum())
            impressions = int(merged_df['è¡¨ç¤ºå›æ•°'].sum())
            ctr         = float(merged_df['CTRï¼ˆ%ï¼‰'].mean())
            position    = float(merged_df['å¹³å‡é †ä½'].mean())
            conversions = int(merged_df['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°'].sum())

        # â€”â€“ ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ & ãƒ†ãƒ¼ãƒ–ãƒ«HTML â€”â€“
        chart_labels = merged_df["URL"].tolist()
        chart_data = {
            "clicks":      merged_df["ã‚¯ãƒªãƒƒã‚¯æ•°"].tolist(),
            "impressions": merged_df["è¡¨ç¤ºå›æ•°"].tolist(),
            "ctr":         merged_df["CTRï¼ˆ%ï¼‰"].tolist(),
            "position":    merged_df["å¹³å‡é †ä½"].tolist(),
            "conversions": merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].tolist()
        }
        table_html   = merged_df.to_html(classes="table table-sm", index=False)

    # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLé¸å®šï¼ˆmerged_df ãŒç©ºãªã‚‰ã‚µã‚¤ãƒˆTOPï¼‰
    if not merged_df.empty:
        worst_page = merged_df.sort_values(by='å¹³å‡é †ä½', ascending=False).iloc[0]
        target_url = worst_page['URL']
    else:
        target_url = site_url.rstrip("/") + "/"
    print(f"ğŸ¯ æ”¹å–„å¯¾è±¡ãƒšãƒ¼ã‚¸: {target_url}")

        # 4. ç«¶åˆæƒ…å ±å–å¾— & ChatGPT æ”¹å–„æ¡ˆå‘¼ã³å‡ºã—ï¼ˆGSCã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹æ™‚ã ã‘ï¼‰
    # --- GSCã®ã€Œã‚¯ã‚¨ãƒªã€åˆ—ãŒã‚ã‚‹å ´åˆã®ã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦åˆ©ç”¨ ---
    gsc_keywords = []
    if (not df_this_week.empty) and ("ã‚¯ã‚¨ãƒª" in df_this_week.columns):
        gsc_keywords = [str(k).strip() for k in df_this_week["ã‚¯ã‚¨ãƒª"].dropna().tolist() if str(k).strip()]

    competitors_info = []
    competitor_data  = []
    response         = ""   # â† ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€Œç”Ÿæˆã—ãªã„ã€

    if gsc_keywords:  # â† â˜…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ1ã¤ã‚‚ç„¡ã‘ã‚Œã° SERP/ChatGPT ã¯ã‚¹ã‚­ãƒƒãƒ—
        # ä»£è¡¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1ã¤ã§SERPï¼ˆå¿…è¦ãªã‚‰ä¸Šä½Nä»¶ãªã©ã«æ‹¡å¼µï¼‰
        top_urls = []
        try:
            top_urls = get_top_competitor_urls(gsc_keywords[0])
        except Exception as e:
            print(f"âš ï¸ SerpAPIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
            top_urls = []

        if top_urls:
            # ç«¶åˆãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«/ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
            for idx, u in enumerate(top_urls, start=1):
                info = get_meta_info_from_url(u)
                competitors_info.append(info)
                competitor_data.append({
                    "URL": u,
                    "ã‚¿ã‚¤ãƒˆãƒ«": info.get("title", ""),
                    "ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³": info.get("description", ""),
                    "position": idx,
                    "title": info.get("title", ""),
                    "url": u
                })

            # ç«¶åˆãŒå–ã‚ŒãŸæ™‚ã ã‘ChatGPT
            try:
                prompt = build_prompt(target_url, competitors_info, merged_df)
                response = get_chatgpt_response(prompt)
                print("ğŸ’¡ ChatGPTæ”¹å–„æ¡ˆ:", response)
            except Exception as e:
                print(f"âš ï¸ ChatGPTã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {e}")
                response = ""  # å¤±æ•—æ™‚ã‚‚ç©ºã®ã¾ã¾
        else:
            print("â„¹ï¸ ç«¶åˆURLãŒå–å¾—ã§ããšã€ChatGPTç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
    else:
        print("â„¹ï¸ GSCã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒç„¡ã„ãŸã‚ã€SERP/ChatGPTã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


    spreadsheet = get_spreadsheet(SPREADSHEET_ID)
    sheet_result = get_or_create_worksheet(spreadsheet, "SEOãƒ‡ãƒ¼ã‚¿")

    print("ğŸ“¤ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")
    update_sheet(sheet_result, merged_df.columns.tolist(), merged_df.values.tolist())
    write_competitor_data_to_sheet(spreadsheet, competitor_data)
    print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
        

    html_rows = ""
    for _, row in merged_df.iterrows():
       html_rows += f"<tr><td>{row['URL']}</td><td>{row['ã‚¯ãƒªãƒƒã‚¯æ•°']}</td><td>{row['è¡¨ç¤ºå›æ•°']}</td><td>{row['CTRï¼ˆ%ï¼‰']}</td><td>{row['å¹³å‡é †ä½']}</td><td>{row['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°']}</td></tr>"

    total_clicks = merged_df['ã‚¯ãƒªãƒƒã‚¯æ•°'].sum()
    total_impressions = merged_df['è¡¨ç¤ºå›æ•°'].sum()
    overall_ctr = (total_clicks / total_impressions) * 100 if total_impressions > 0 else 0
    average_position = merged_df['å¹³å‡é †ä½'].mean()
    total_conversions = merged_df['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°'].sum()

    clicks      = int(total_clicks)
    impressions = int(total_impressions)
    ctr         = float(overall_ctr)
    position    = float(average_position)
    conversions = int(total_conversions)
    table_html   = html_rows
    chart_labels = merged_df["URL"].tolist()
    data = {
        "clicks":      merged_df["ã‚¯ãƒªãƒƒã‚¯æ•°"].tolist(),
        "impressions": merged_df["è¡¨ç¤ºå›æ•°"].tolist(),
        "ctr":         merged_df["CTRï¼ˆ%ï¼‰"].tolist(),
        "position":    merged_df["å¹³å‡é †ä½"].tolist(),
        "conversions": merged_df["ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°"].tolist()
    }

    return {
        "clicks":           clicks,
        "impressions":      impressions,
        "ctr":              ctr,
        "position":         position,
        "conversions":      conversions,
        "table_html":       table_html,
        "chart_labels":     chart_labels,
        "chart_data":       data,
        "competitors":      competitor_data,
        "chatgpt_response": response or ""
    }

def get_history_for_user(uid):
    docs = (
        db.collection("improvements")
          .where("uid", "==", uid)
          .order_by("timestamp", direction=firestore.Query.DESCENDING)
          .stream()
    )
    history = []
    for doc in docs:
        d = doc.to_dict()
        history.append({
            "id":               doc.id,
            "timestamp":        d["timestamp"].strftime("%Y-%m-%d %H:%M:%S"),
            "input_url":        d["input_url"],
            "chatgpt_response": d["result"]["chatgpt_response"]
        })
    return history

if __name__ == "__main__":
    process_seo_improvement("sc-domain:mrseoai.com")
